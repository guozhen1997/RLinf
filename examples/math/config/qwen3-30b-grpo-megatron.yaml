defaults:
  - override hydra/job_logging: stdout

hydra:
  run:
    dir: .
  output_subdir: null

cluster:
  num_nodes: 1
  num_gpus_per_node: 8
  component_placement:
    actor,rollout: all

runner:
  task_type: math
  logger:
    log_path: ${runner.output_dir}/${runner.experiment_name}
    project_name: rlinf
    experiment_name: ${runner.experiment_name}
    logger_backends: ["tensorboard"] # wandb, swanlab

  max_epochs: 5
  max_steps: -1

  val_check_interval: 1
  save_interval: 50

  seq_length: 512
  # num_experts: 8

  enable_dynamic_batch_size: False
  max_tokens_per_mbs: 512

  resume_dir: null
  experiment_name: grpo-1.5b
  output_dir: ../results

algorithm:
  group_size: 4

  n_minibatches: 4
  training_batch_size_per_gpu: 1 # micro batch size
  rollout_batch_size_per_gpu: null # If set to null, rollout_batch_size will be evenly divided across all inference instances. You can reduce this parameter if inference consumes too much GPU memory.

  # mbs to do log prob inference, can be set to
  # lower than rollout_batch_size_per_gpu to reduce
  # memory usage
  logprob_forward_micro_batch_size: 1 # ${.rollout_batch_size_per_gpu}

  # val rollout mbs
  val_rollout_batch_size_per_gpu: 4 # ${.rollout_batch_size_per_gpu}

  recompute_logprobs: True
  shuffle_rollout: False

  # GRPO loss params
  loss_type: ppo
  loss_agg_func: "token-mean"
  kl_beta: 0.0 # 0.001
  kl_penalty_type: low_var_kl
  ratio_clip_eps: 0.2
  entropy_bonus: 0.0
  calculate_entropy: False
  clip_ratio_c: null # 3.0

  adv_type: grpo
  normalize_advantages: True
  early_stop_imp_ratio: 5.0
  use_valid_token_scale: False

  # params for rollout
  sampling_params:
    use_greedy: False
    temperature: 1.0
    top_k: 1000000
    top_p: 1.0
    repetition_penalty: 1.0
    max_new_tokens: ${subtract:${runner.seq_length}, ${data.max_prompt_length}}
    min_new_tokens: 1

rollout:
  group_name: "RolloutGroup"

  gpu_memory_utilization: 0.3

  model_dir: /mnt/public/rl_fuxu/example_qwen3/qwen3_30b_a3b_hf/
  model_arch: qwen3-30b
  enforce_eager: False         # if False, vllm will capture cuda graph, which will take more time to initialize.
  distributed_executor_backend: mp   # ray or mp
  disable_log_stats: False
  detokenize: False            # Whether to detokenize the output. During RL we actually don't need to detokenize it. Can be set to True for debugging.
  padding: null               # will be tokenizer.pad_token_id if null. it is used to filter megatron's padding for vllm rollout
  eos: null                   # will be tokenizer.eos_token_id if null.

  attention_backend: triton

  return_logprobs: ${not:${algorithm.recompute_logprobs}}

  tensor_parallel_size: 4
  pipeline_parallel_size: 1
  
  validate_weight: False # whether to send all weights at first for weight comparison.
  validate_save_dir: null # the directory to save the weights for comparison. If validate_weight is True, this will be used to save the weights for comparison.
  print_outputs: False         # whether to print the outputs (token ids, texts, etc.) of inference engine.

  sglang_decode_log_interval: 500000 # the interval for SGLang to log the decode time and other stats.
  max_running_requests: 64 # the maximum number of running requests in the inference engine.
  cuda_graph_max_bs: 128 # the maximum batch size for cuda graph. If the batch size is larger than this, cuda graph will not be used.

  use_torch_compile: False # enable torch_compile in SGLang for rollout.
  torch_compile_max_bs: 128 # the maximum batch size for torch compile. If the batch size is larger than this, torch compile will not be used.

data:
  type: math
  max_prompt_length: 256
  filter_prompt_by_length: True
  rollout_batch_size: 16
  val_rollout_batch_size: null
  num_workers: 2
  prompt_key: prompt
  shuffle: True
  validation_shuffle: True
  seed: 1234
  train_data_paths: ["/mnt/public/rl_fuxu/data/boba_106k_0319_prompt_1024.jsonl"]
  val_data_paths: ["/mnt/public/rl_fuxu/data/boba_106k_0319_prompt_1024.jsonl"]

actor:
  group_name: "ActorGroup"
  training_backend: megatron
  mcore_gpt: True
  spec_name: decoder_gpt

  checkpoint_load_path: /mnt/public/rl_fuxu/example_qwen3/qwen3_30b_a3b_mg_tp4_tpe4_ep1_pp2

  offload_optimizer: True
  offload_weight: True
  offload_grad: True

  enable_dp_load_balance: False

  calculate_flops: False

  seed: 1234

  model:
    precision: fp16
    add_bias_linear: False

    tensor_model_parallel_size: 4
    pipeline_model_parallel_size: 2
    expert_model_parallel_size: 1
    expert_tensor_parallel_size: 4

    activation: swiglu
    sequence_parallel: True
    # recompute_method: block
    # recompute_granularity: selective

    recompute_method: uniform
    recompute_granularity: full
    recompute_num_layers: 1

    seq_length: ${runner.seq_length}
    encoder_seq_length: ${runner.seq_length}

    normalization: rmsnorm

    position_embedding_type: rope

    apply_rope_fusion: True
    bias_dropout_fusion: False
    persist_layer_norm: False
    bias_activation_fusion: True
    attention_softmax_in_fp32: True
    batch_p2p_comm: False
    variable_seq_lengths: True
    gradient_accumulation_fusion: True
    moe_token_dispatcher_type: alltoall
    use_cpu_initialization: False

    # MoE related
    moe_router_load_balancing_type: "aux_loss"
    init_method_std: 0.008
    moe_router_topk: 2
    moe_router_pre_softmax: True
    moe_aux_loss_coeff: 0.0

    # qwen3 related
    qk_layernorm: True

  optim:
    optimizer: adam
    bf16: False
    fp16: True
    lr: 2e-05
    adam_beta1: 0.9
    adam_beta2: 0.95
    adam_eps: 1.0e-05
    min_lr: 2.0e-6
    weight_decay: 0.05
    use_distributed_optimizer: True
    overlap_grad_reduce: True
    overlap_param_gather: True
    optimizer_enable_pin: false
    overlap_param_gather_with_optimizer_step: False
    clip_grad: 0.8
    loss_scale: 65536

  lr_sched:
    lr_warmup_fraction: 0.01
    lr_warmup_init: 0.0
    lr_warmup_iters: 0
    max_lr: 2.0e-5
    min_lr: 0.0
    lr_decay_style: constant
    lr_decay_iters: 10

  tokenizer:
    tokenizer_model: /mnt/public/rl_fuxu/example_qwen3/qwen3_30b_a3b_hf/
    use_fast: False
    trust_remote_code: True
    padding_side: 'right'

  megatron:
    ddp_bucket_size: null
    distributed_backend: nccl # Support 'nccl' and 'gloo'
    distributed_timeout_minutes: 30
    ckpt_format: torch
    use_dist_ckpt: False
    tp_comm_bootstrap_backend: nccl
    tp_comm_overlap_cfg: null # tp_comm_overlap_cfg.yaml
    use_hf_ckpt: False # if true, will transfer hf model to generate megatron checkpoint and use it for training.
    use_profiler: False # if true, will enable torch profiler when training, pay attention it has influence on performance

    ckpt_convertor: # config for ckpt convertor
      model: Qwen3-30B-A3B
      model_type: null # will be set by hf model's config if null
      hf_model_path: ${rollout.model_dir} # path to the hf model
      save_path: ${runner.output_dir}/${runner.experiment_name}/converted_ckpts/actor
      use_gpu_num : 0
      use_gpu_index: null
      process_num: 8 # number of processes to use for checkpointing
      tensor_model_parallel_size: ${actor.model.tensor_model_parallel_size}
      pipeline_model_parallel_size: ${actor.model.pipeline_model_parallel_size}
      expert_model_parallel_size: ${actor.model.expert_model_parallel_size}
      expert_tensor_parallel_size: ${actor.model.expert_tensor_parallel_size}
      
    # profiler: # profile megatron when inference and traning
    #   output_dir: ${runner.output_dir}/${runner.experiment_name}/profiler
    #   activities: ["cpu", "cuda"]
    #   record_shapes: False
    #   profile_memory: False
    #   with_stack: False
    #   with_flops: False
    #   with_modules: True
    #   export_tensorboard: True
    #   export_chrome_trace: False
    #   chrome_filename_prefix: "chrome_trace"
    #   schedule_warmup: 2
    #   schedule_active: 1
    #   schedule_repeat: 1 # inference and training will repeat such times
    #   # schedule_wait: it will be set at runtime


reward:
  use_reward_model: false
  reward_type: 'math'
  reward_scale: 5.0

critic:
  use_critic_model: false