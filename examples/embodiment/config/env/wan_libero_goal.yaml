env_type: wan_wm
task_suite_name: libero_goal
wm_env_type: libero

total_num_envs: null

auto_reset: False
ignore_terminations: False
max_steps_per_rollout_epoch: 240
max_episode_steps: 240 # max episode steps for truncation

use_rel_reward: True
reward_coef: 1.0

# RLinf LiberoEnv specific settings
reset_gripper_open: True
is_eval: False

seed: 0
group_size: 1
use_fixed_reset_state_ids: True
use_ordered_reset_state_ids: False

# Indicate the reset state id (related to the task_id and trial_id) want to be specified. (e.g. 
# if set 0, the env always use the task 0 and trial 0 in the libero-goal suite; 
# if set null, the initial states will be sampled from the whole libero-goal suite. )
specific_reset_id: null # can be a number or null.

video_cfg:
  save_video: True
  info_on_video: True
  video_base_dir: ${runner.logger.log_path}/video/train

enable_offload: False

###  ------------------Wan params define here ------------------

wan_wm_hf_ckpt_path: null

VAE_path: ${env.train.wan_wm_hf_ckpt_path}/Wan2.2_VAE.pth
model_path: ${env.train.wan_wm_hf_ckpt_path}/model-00001.safetensors

# enable KIR Trick
enable_kir: True # keyframe-int rollout
initial_image_path: ${env.train.wan_wm_hf_ckpt_path}/dataset/

chunk: 8                     # should be same as chunk in VLA
condition_frame_length: 5    # condition frame length of world model
image_size: [256, 256]       # output image size of world model
num_frames: 13               # total number of frames to encode = condition_frame_length + chunk

reward_model:
  type: TaskEmbedResnetRewModel
  from_pretrained: ${env.train.wan_wm_hf_ckpt_path}/taskemb_resnet_rm.pth